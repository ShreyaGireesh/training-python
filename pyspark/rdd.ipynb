{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ed023b-76f8-4f4d-bedf-259adaadd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248a9e7d-071f-4974-9514-2ba593a7e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x75f20021e710>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 11:22:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5911e-2bf4-444a-8485-5f85e5b80063",
   "metadata": {},
   "source": [
    "RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8683b4ce-f771-4444-9c60-57942ab36458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 28)]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 28)])\n",
    "print(rdd.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a921d0-8f0a-4bb3-ac10-ac94b4d52044",
   "metadata": {},
   "source": [
    "Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd74a58-041f-45a7-9fbd-4be2544d5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| ID|   Name|Age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 28|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 28)], [\"ID\", \"Name\", \"Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50b684f-fa86-4e02-b181-17b6ef937d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID=1, Name='Alice', Age=25), Row(ID=2, Name='Bob', Age=30), Row(ID=3, Name='Charlie', Age=28)]\n"
     ]
    }
   ],
   "source": [
    "# converting dataframe to rdd\n",
    "\n",
    "rdd = df.rdd\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a595b4e-ea17-4cf3-b6ce-d7f42eabf1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice,25,New York', 'Bob,30,San Francisco', 'Charlie,28,Los Angeles']\n"
     ]
    }
   ],
   "source": [
    "# reading from a txt file\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"data.txt\")\n",
    "print(rdd.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51530a56-08df-4a1a-bae2-fbc4b7d8c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name,age,city', 'Alice,25,New York', 'Bob,30,San Francisco', 'Charlie,28,Los Angeles', 'David,35,Chicago', 'Eve,22,Houston']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"data.csv\")  \n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4bb746-2c12-411f-ae71-4a4efdcdd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['name', 'age', 'city'], ['Alice', '25', 'New York'], ['Bob', '30', 'San Francisco'], ['Charlie', '28', 'Los Angeles'], ['David', '35', 'Chicago'], ['Eve', '22', 'Houston']]\n"
     ]
    }
   ],
   "source": [
    "rdd_split = rdd.map(lambda line: line.split(\",\"))  \n",
    "print(rdd_split.collect())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbfc897-0d93-41ac-b969-db1f6170a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Alice', 'age': 25, 'city': 'New York'}, {'name': 'Bob', 'age': 30, 'city': 'San Francisco'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "rdd = spark.sparkContext.textFile(\"data.json\")  \n",
    "json_rdd = rdd.map(lambda x: json.loads(x))  \n",
    "print(json_rdd.collect())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad21462a-4982-4498-b069-824bcfd904d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project Gutenberg’s', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'Project Gutenberg’s', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'This eBook is for the use', 'of anyone anywhere', 'at no cost and with', 'Project Gutenberg’s', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"test.txt\")\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bf58201-5dbe-48b2-818b-9ce3b63c60c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg’s', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'Project', 'Gutenberg’s', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'Project', 'Gutenberg’s', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll']\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201f7dfb-04db-4dd9-a6b3-de6e1f9e41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Project', 1), ('Gutenberg’s', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('Project', 1), ('Gutenberg’s', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('Project', 1), ('Gutenberg’s', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef651d75-5082-466a-8778-6a5699423de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Project', 3), ('Gutenberg’s', 3), ('Alice’s', 5), ('in', 5), ('Lewis', 5), ('Carroll', 5), ('is', 6), ('use', 6), ('of', 6), ('anyone', 6), ('anywhere', 6), ('at', 6), ('no', 6), ('Adventures', 5), ('Wonderland', 5), ('by', 5), ('This', 6), ('eBook', 6), ('for', 6), ('the', 6), ('cost', 6), ('and', 6), ('with', 6)]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda a,b: a+b)\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66158105-8b7b-4347-a330-dbc0b52f1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'Project'), (3, 'Gutenberg’s'), (5, 'Alice’s'), (5, 'in'), (5, 'Lewis'), (5, 'Carroll'), (5, 'Adventures'), (5, 'Wonderland'), (5, 'by'), (6, 'is'), (6, 'use'), (6, 'of'), (6, 'anyone'), (6, 'anywhere'), (6, 'at'), (6, 'no'), (6, 'This'), (6, 'eBook'), (6, 'for'), (6, 'the'), (6, 'cost'), (6, 'and'), (6, 'with')]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c02259-5dbf-44ca-a9fd-13a83ddde7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 'is'), (6, 'use'), (6, 'of'), (6, 'anyone'), (6, 'anywhere'), (6, 'at'), (6, 'no'), (6, 'This'), (6, 'eBook'), (6, 'for'), (6, 'the'), (6, 'cost'), (6, 'and'), (6, 'with'), (5, 'Alice’s'), (5, 'in'), (5, 'Lewis'), (5, 'Carroll'), (5, 'Adventures'), (5, 'Wonderland'), (5, 'by'), (3, 'Project'), (3, 'Gutenberg’s')]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "583b9a68-c3c0-4961-b8be-e78daf447462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice’s', 5), ('in', 5), ('Lewis', 5), ('Carroll', 5), ('is', 6), ('use', 6), ('of', 6), ('anyone', 6), ('anywhere', 6), ('at', 6), ('no', 6), ('Adventures', 5), ('Wonderland', 5), ('by', 5), ('This', 6), ('eBook', 6), ('for', 6), ('the', 6), ('cost', 6), ('and', 6), ('with', 6)]\n"
     ]
    }
   ],
   "source": [
    "filtered_rdd = rdd4.filter(lambda x: x[1] > 3)\n",
    "print(filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c61eddb6-eca2-42ed-95db-02137078962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg’s', 'Alice’s', 'in', 'Lewis', 'Carroll', 'is', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'Adventures', 'Wonderland', 'by', 'This', 'eBook', 'for', 'the', 'cost', 'and', 'with']\n"
     ]
    }
   ],
   "source": [
    "distinct_words = rdd2.distinct()\n",
    "print(distinct_words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcce1560-a28e-4908-a349-7f2fe4d91c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "total_words = rdd2.count()\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c37f65fb-1d5a-4d94-983c-5922fce3802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Count : \"+str(rdd5.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfe197c7-5f72-457e-9383-dbdc3cc51297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Record : 6,is\n"
     ]
    }
   ],
   "source": [
    "firstRec = rdd5.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a50d281-bfdf-497f-aa18-f2559590980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataReduce Record : 125\n"
     ]
    }
   ],
   "source": [
    "totalWordCount = rdd5.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91c28552-7247-4d41-bf00-bb70534b7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 11:19:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e063dd1c-0ff4-4bb8-8431-c315aac758ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y: x + y)\n",
    "combOp = (lambda x, y: x + y)\n",
    "agg=listRdd.aggregate(0, seqOp, combOp)\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97003c5c-44f6-4f8a-b5d5-ddc6d1151ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/05 21:19:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(101, 'Alice', 'HR', 5000, 5), (102, 'Bob', 'IT', 7000, 3), (103, 'Charlie', 'Finance', 6500, 8), (104, 'David', 'HR', 4800, 2), (105, 'Eve', 'IT', 7200, 7), (106, 'Frank', 'Finance', 6400, 6), (107, 'Grace', 'IT', 7300, 10), (108, 'Helen', 'HR', 5200, 4), (109, 'Ivan', 'Finance', 6000, 3), (110, 'Jack', 'IT', 7500, 9)]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"RDDOperationsExample\").getOrCreate()\n",
    "# Employee data (ID, Name, Department, Salary, Experience in Years)\n",
    "employee_data = [\n",
    "    (101, \"Alice\", \"HR\", 5000, 5),\n",
    "    (102, \"Bob\", \"IT\", 7000, 3),\n",
    "    (103, \"Charlie\", \"Finance\", 6500, 8),\n",
    "    (104, \"David\", \"HR\", 4800, 2),\n",
    "    (105, \"Eve\", \"IT\", 7200, 7),\n",
    "    (106, \"Frank\", \"Finance\", 6400, 6),\n",
    "    (107, \"Grace\", \"IT\", 7300, 10),\n",
    "    (108, \"Helen\", \"HR\", 5200, 4),\n",
    "    (109, \"Ivan\", \"Finance\", 6000, 3),\n",
    "    (110, \"Jack\", \"IT\", 7500, 9),\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(employee_data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0033f5b3-914a-4b90-ae1a-edcf2ba4b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(101, 'Alice', 'HR', 5500.0, 5), (102, 'Bob', 'IT', 7700.000000000001, 3), (103, 'Charlie', 'Finance', 7150.000000000001, 8), (104, 'David', 'HR', 5280.0, 2), (105, 'Eve', 'IT', 7920.000000000001, 7), (106, 'Frank', 'Finance', 7040.000000000001, 6), (107, 'Grace', 'IT', 8030.000000000001, 10), (108, 'Helen', 'HR', 5720.000000000001, 4), (109, 'Ivan', 'Finance', 6600.000000000001, 3), (110, 'Jack', 'IT', 8250.0, 9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# increase salary by 10%\n",
    "rdd_map = rdd.map(lambda x: (x[0], x[1], x[2], x[3] * 1.1, x[4]))\n",
    "print(rdd_map.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e4e6f4-bfec-452f-bda0-ff0c239f004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(103, 'Charlie', 'Finance', 6500, 8), (105, 'Eve', 'IT', 7200, 7), (106, 'Frank', 'Finance', 6400, 6), (107, 'Grace', 'IT', 7300, 10), (110, 'Jack', 'IT', 7500, 9)]\n"
     ]
    }
   ],
   "source": [
    "# employees with more than 5 year experience\n",
    "rdd_filter = rdd.filter(lambda x: x[4] > 5)\n",
    "print(rdd_filter.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761fe5e4-9f5b-4da6-936e-a9d4aea7701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HR', 'IT', 'Finance', 'HR', 'IT', 'Finance', 'IT', 'HR', 'Finance', 'IT']\n"
     ]
    }
   ],
   "source": [
    "# flatten department into words\n",
    "rdd_flatmap = rdd.flatMap(lambda x: x[2].split(\" \"))\n",
    "print(rdd_flatmap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a95c240-ff5a-4a01-9098-d9a920ca1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Finance': ['Charlie', 'Frank', 'Ivan'], 'HR': ['Alice', 'David', 'Helen'], 'IT': ['Bob', 'Eve', 'Grace', 'Jack']}\n"
     ]
    }
   ],
   "source": [
    "# group by department\n",
    "rdd_kv = rdd.map(lambda x: (x[2], x[1]))  \n",
    "rdd_grouped = rdd_kv.groupByKey()\n",
    "print({k: list(v) for k, v in rdd_grouped.collect()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d198f5-53e1-48c1-b92b-601e59cc854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Finance', 18900), ('HR', 15000), ('IT', 29000)]\n"
     ]
    }
   ],
   "source": [
    "# total salary in each department\n",
    "rdd_salary_kv = rdd.map(lambda x: (x[2], x[3])) \n",
    "rdd_salary_total = rdd_salary_kv.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd_salary_total.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44067f96-6f55-408a-a481-1e4daa64c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7500, (110, 'Jack', 'IT', 7500, 9)), (7300, (107, 'Grace', 'IT', 7300, 10)), (7200, (105, 'Eve', 'IT', 7200, 7)), (7000, (102, 'Bob', 'IT', 7000, 3)), (6500, (103, 'Charlie', 'Finance', 6500, 8)), (6400, (106, 'Frank', 'Finance', 6400, 6)), (6000, (109, 'Ivan', 'Finance', 6000, 3)), (5200, (108, 'Helen', 'HR', 5200, 4)), (5000, (101, 'Alice', 'HR', 5000, 5)), (4800, (104, 'David', 'HR', 4800, 2))]\n"
     ]
    }
   ],
   "source": [
    "# sort by salary\n",
    "rdd_sorted = rdd.map(lambda x: (x[3], x)).sortByKey(ascending=False)\n",
    "print(rdd_sorted.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259cee4b-301c-4b27-85af-4af60c59bcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finance', 'HR', 'IT']\n"
     ]
    }
   ],
   "source": [
    "# unique department\n",
    "rdd_distinct = rdd.map(lambda x: x[2]).distinct()\n",
    "print(rdd_distinct.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ce1042-f0a8-40db-93b9-40f4fffd44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(101, 'Alice', 'HR', 5000, 5), (102, 'Bob', 'IT', 7000, 3), (103, 'Charlie', 'Finance', 6500, 8), (104, 'David', 'HR', 4800, 2), (105, 'Eve', 'IT', 7200, 7), (106, 'Frank', 'Finance', 6400, 6), (107, 'Grace', 'IT', 7300, 10), (108, 'Helen', 'HR', 5200, 4), (109, 'Ivan', 'Finance', 6000, 3), (110, 'Jack', 'IT', 7500, 9), (111, 'Karen', 'IT', 6800, 5), (112, 'Liam', 'HR', 5700, 6)]\n"
     ]
    }
   ],
   "source": [
    "# add 2 employees\n",
    "extra_employees = [(111, \"Karen\", \"IT\", 6800, 5), (112, \"Liam\", \"HR\", 5700, 6)]\n",
    "rdd_extra = spark.sparkContext.parallelize(extra_employees)\n",
    "rdd_union = rdd.union(rdd_extra)\n",
    "print(rdd_union.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0313f77-6cf4-4c09-b02f-25227cb90afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:====================================>                     (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(104, (('David', 'HR', 4800), 300)), (101, (('Alice', 'HR', 5000), 500)), (102, (('Bob', 'IT', 7000), 800)), (103, (('Charlie', 'Finance', 6500), 600))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# add bonus\n",
    "bonus_data = [(101, 500), (102, 800), (103, 600), (104, 300)]\n",
    "rdd_bonus = spark.sparkContext.parallelize(bonus_data)\n",
    "rdd_emp_kv = rdd.map(lambda x: (x[0], (x[1], x[2], x[3])))  \n",
    "rdd_joined = rdd_emp_kv.join(rdd_bonus)\n",
    "print(rdd_joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85d0004-b0c6-430e-9d8c-8795efb469ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12000, 11300, 13600, 26000]\n"
     ]
    }
   ],
   "source": [
    "def salary_sum(iterator):\n",
    "    yield sum(x[3] for x in iterator)\n",
    "\n",
    "rdd_partitioned = rdd.mapPartitions(salary_sum)\n",
    "print(rdd_partitioned.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e15a2e-9662-4ba8-9637-7ef033b46830",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'employee_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m names_rdd \u001b[38;5;241m=\u001b[39m \u001b[43memployee_rdd\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Extracting Names\u001b[39;00m\n\u001b[1;32m      2\u001b[0m departments_rdd \u001b[38;5;241m=\u001b[39m employee_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# Extracting Departments\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Zipping both RDDs\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'employee_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "names_rdd = employee_rdd.map(lambda x: x[1])  # Extracting Names\n",
    "departments_rdd = employee_rdd.map(lambda x: x[2])  # Extracting Departments\n",
    "\n",
    "# Zipping both RDDs\n",
    "zipped_rdd = names_rdd.zip(departments_rdd)\n",
    "\n",
    "# Collect and print the results\n",
    "print(zipped_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b30bc8b-bff2-4cc0-a4d3-0d370bf9c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c4fb285-c92a-49dd-9fea-5af24813baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62900\n"
     ]
    }
   ],
   "source": [
    "total_salary = rdd.map(lambda x: x[3]).reduce(lambda a, b: a + b)\n",
    "print(total_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75a5d286-9275-484b-9208-5c39bf0225c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 'Alice', 'HR', 5000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e10b913-3159-4e11-ad03-6ae56f0fcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"output_employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5de8faf5-6cd6-4062-afeb-babba448ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "rdd.cache()\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44251b8d-cef3-4925-9254-2915c13f8a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rdd_coalesce = rdd.coalesce(2)\n",
    "print(rdd_coalesce.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a084720c-e307-458a-917b-1271a4c172eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "rdd_repartition = rdd.repartition(4)\n",
    "print(rdd_repartition.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8f6b-6ea5-4f2c-bb56-f5c508d9193b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
