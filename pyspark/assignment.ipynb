{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f01aad-245e-4a20-a343-c5451adaaaf4",
   "metadata": {},
   "source": [
    "__RDD__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433948f-3da5-48bc-9d21-e92ad67167e5",
   "metadata": {},
   "source": [
    "__Create an RDD from the given dataset and display its first 5 elements.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebbe7dc-86c5-48d7-bd47-8d007a07afdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 11:02:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice 50000 HR', 'Bob 60000 IT', 'Charlie 55000 Finance', 'David 45000 HR', 'Eve 70000 IT']\n",
      "['Alice 50000 HR', 'Bob 60000 IT', 'Charlie 55000 Finance', 'David 45000 HR', 'Eve 70000 IT']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"assignment\").getOrCreate()\n",
    "\n",
    "rdd_data = [\n",
    "\"Alice 50000 HR\",\n",
    "\"Bob 60000 IT\",\n",
    "\"Charlie 55000 Finance\",\n",
    "\"David 45000 HR\",\n",
    "\"Eve 70000 IT\"\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(rdd_data)\n",
    "print(rdd.collect())\n",
    "print(rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573c076-b906-40bc-b9f6-4f242b613928",
   "metadata": {},
   "source": [
    "__Split each line in an RDD into words using flatMap().__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ffd55eb-a416-407d-9895-f2142b535f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', '50000', 'HR', 'Bob', '60000', 'IT', 'Charlie', '55000', 'Finance', 'David', '45000', 'HR', 'Eve', '70000', 'IT']\n"
     ]
    }
   ],
   "source": [
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db7cdd-4807-4c5a-ba26-a487c0dd571e",
   "metadata": {},
   "source": [
    "__Count the occurrences of each word in an RDD.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf25a4c9-9360-42b7-8dd5-b049eb7a78af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 2, 9, 5]\n"
     ]
    }
   ],
   "source": [
    "num_rdd_data = [3, 6, 2, 9, 5]\n",
    "rdd1 = spark.sparkContext.parallelize(num_rdd_data)\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18aeb26-8c35-4f0c-bb9e-4249db3872ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 1), (5, 1), (6, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "num_count = rdd1.map(lambda no: (no, 1))\n",
    "result = num_count.reduceByKey(lambda a,b: a+b)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988ee5e-ac8b-4c44-a5aa-164bc2b4cd5c",
   "metadata": {},
   "source": [
    "__Find the maximum value in an RDD of numbers.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0c9788-25b0-438f-bd71-ab8be019d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "max_value = rdd1.reduce(lambda a,b:a if a>b else b)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352da296-affe-42d3-a11c-bd8113be2b68",
   "metadata": {},
   "source": [
    "__Sort an RDD in descending order.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e5ed67-7bc9-439e-9d0e-b11171c1a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "sort_rdd = rdd1.sortBy(lambda x:x, ascending=False)\n",
    "print(sort_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615df635-d75c-4228-be4b-78a54e07700b",
   "metadata": {},
   "source": [
    "__Dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e6a4b13-02f3-47f5-9414-9568f069ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c792df-d14b-4d44-bb24-cdb99d984bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|\n",
      "|  2|    Bob|        IT|   NULL|2017-08-15|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|\n",
      "|  4|  David|        HR|   NULL|2020-09-12|\n",
      "|  5|    Eve|        IT|70000.0|      NULL|\n",
      "|  6|   NULL|     Sales|48000.0|2019-03-22|\n",
      "|  7|  Grace|      NULL|52000.0|2021-01-10|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkAssignment\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "(1, \"Alice\", \"HR\", 50000.0, \"2018-07-10\"),\n",
    "(2, \"Bob\", \"IT\", None, \"2017-08-15\"),\n",
    "(3, \"Charlie\", \"Finance\", 55000.0, \"2019-06-25\"),\n",
    "(4, \"David\", \"HR\", None, \"2020-09-12\"),\n",
    "(5, \"Eve\", \"IT\", 70000.0, None),\n",
    "(6, None, \"Sales\", 48000.0, \"2019-03-22\"),\n",
    "(7, \"Grace\", None, 52000.0, \"2021-01-10\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"ID\", IntegerType(), True),\n",
    "StructField(\"Name\", StringType(), True),\n",
    "StructField(\"Department\", StringType(), True),\n",
    "StructField(\"Salary\", DoubleType(), True),\n",
    "StructField(\"Hire_Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab48982-a8e3-4ffe-91ea-786b2232f3b7",
   "metadata": {},
   "source": [
    "__Find the distinct values of salary column in a DataFrame.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34352e49-6556-49a1-a5c3-86a7d94988be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Salary|\n",
      "+-------+\n",
      "|50000.0|\n",
      "|55000.0|\n",
      "|   NULL|\n",
      "|70000.0|\n",
      "|48000.0|\n",
      "|52000.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Salary\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38057c1-5c5f-4411-82d7-09080e255e96",
   "metadata": {},
   "source": [
    "__Replace null values in a DataFrame column salary with a default value 40000__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0f577dc-ed31-4612-a0f3-a8c6d91616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|\n",
      "|  5|    Eve|        IT|70000.0|      NULL|\n",
      "|  6|   NULL|     Sales|48000.0|2019-03-22|\n",
      "|  7|  Grace|      NULL|52000.0|2021-01-10|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changed_df = df.fillna({\"Salary\":40000.0})\n",
    "changed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb4cd9-3f44-4c9f-b6e1-05784c5cf0e4",
   "metadata": {},
   "source": [
    "__Drop rows with missing values in a DataFrame.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45ddfb48-2d7c-4421-9f69-0ac2162787a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = changed_df.dropna()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300458b-e783-49e1-9309-cb0ccf52cfe7",
   "metadata": {},
   "source": [
    "__Convert a DataFrame column Name to uppercase.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29b0710a-bc11-4893-8424-54c4b3c75a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  1|  ALICE|        HR|50000.0|2018-07-10|\n",
      "|  2|    BOB|        IT|40000.0|2017-08-15|\n",
      "|  3|CHARLIE|   Finance|55000.0|2019-06-25|\n",
      "|  4|  DAVID|        HR|40000.0|2020-09-12|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upper_df = new_df.withColumn(\"Name\", upper(new_df[\"Name\"]))\n",
    "upper_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df6836-946d-443e-81f8-7f4ac7ebcbb0",
   "metadata": {},
   "source": [
    "__find the average salary per department__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d77185b5-abdf-4688-a0ff-3d2da59f8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5a280cf-7691-4474-b48e-de98a0fbde68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|Department|Avg Salary|\n",
      "+----------+----------+\n",
      "|        HR|   45000.0|\n",
      "|   Finance|   55000.0|\n",
      "|        IT|   40000.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Avg Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fef63e-5c02-4b72-bc0f-f27e63be0683",
   "metadata": {},
   "source": [
    "__Extract the year from a date column in a DataFrame.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "651ca661-39fc-42f0-aed2-b39e817e3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b227d4ff-7da4-41d9-923d-ab240b34f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+---------+\n",
      "| ID|   Name|Department| Salary|Hire_Date|\n",
      "+---+-------+----------+-------+---------+\n",
      "|  1|  Alice|        HR|50000.0|     2018|\n",
      "|  2|    Bob|        IT|40000.0|     2017|\n",
      "|  3|Charlie|   Finance|55000.0|     2019|\n",
      "|  4|  David|        HR|40000.0|     2020|\n",
      "+---+-------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convert_df = new_df.withColumn(\"Hire_Date\", new_df[\"Hire_Date\"].cast(\"date\"))\n",
    "year_df = convert_df.withColumn(\"Hire_Date\", year(convert_df[\"Hire_Date\"]))\n",
    "year_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ecc43-56c3-487c-80c2-b050aa4af0fd",
   "metadata": {},
   "source": [
    "__Sort a DataFrame by multiple columns — sort the DataFrame in ascending order by the department column and in descending order by the salary column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b20889b8-2a05-4cc7-8671-ed0a510f9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb994bc5-770f-4916-a49a-a09aeb19e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.orderBy(col(\"Department\"),col(\"Salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7deccc-e9af-4f73-9a2e-8467ace210eb",
   "metadata": {},
   "source": [
    "__Extract the first 3 characters from a Name column and display__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2d4b884-84dd-45fe-839d-626e33c66fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf3070ad-5bc7-4ce9-a472-e558a7e062de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+----------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|Name_3char|\n",
      "+---+-------+----------+-------+----------+----------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|       Ali|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|       Bob|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|       Cha|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|       Dav|\n",
      "+---+-------+----------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.withColumn(\"Name_3char\", new_df[\"Name\"].substr(1, 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ae6fe-615e-4814-ba7c-1db6614810e6",
   "metadata": {},
   "source": [
    "__Check if Name column contains a substring ‘lic’__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02a68f08-b2e5-4e00-aee8-8fe412f492f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------+----------+\n",
      "| ID| Name|Department| Salary| Hire_Date|\n",
      "+---+-----+----------+-------+----------+\n",
      "|  1|Alice|        HR|50000.0|2018-07-10|\n",
      "+---+-----+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.filter(col(\"Name\").rlike(\"(?i)lic\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9e148-f70d-448c-a2db-86e4a69b7bd8",
   "metadata": {},
   "source": [
    "__Get the row count of a DataFrame.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d48dfef-2898-49cc-b431-ba8838a35a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4\n"
     ]
    }
   ],
   "source": [
    "row_count = new_df.count()\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab0617-bff6-4673-b953-de5fb1d35abc",
   "metadata": {},
   "source": [
    "__Write this DataFrame to a CSV file.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e595f96-0b36-4928-af0d-877bc8c35500",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.csv(\"assignment_output\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8bfa5-27ed-4a20-8607-a667a9d6628b",
   "metadata": {},
   "source": [
    "__Apply a User-Defined Function (UDF) to categorize employees based on their salary. Introduce a new column, Salary_Category, where:\n",
    "If the salary is less than 50,000, it should be categorized as \"Low\".\n",
    "If the salary is greater than 50,000 but less than 100,000, it should be categorized as \"Medium\".\n",
    "If the salary is greater than 100,000, it should be categorized as \"High\".I\n",
    "f the salary is None, it should return \"Unknown\".\n",
    "How would you implement this logic in PySpark using a UDF?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f3c44eb-27ce-45e7-a2ea-b0bedc532d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01d48be7-5c6a-406b-8877-b5d7a40943cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+---------------+\n",
      "| ID|   Name|Department| Salary| Hire_Date|Salary_Category|\n",
      "+---+-------+----------+-------+----------+---------------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|         Medium|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|            Low|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|         Medium|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|            Low|\n",
      "+---+-------+----------+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def categorize_salary(salary):\n",
    "    if salary < 50000:\n",
    "        return \"Low\"\n",
    "    elif salary>=50000 and salary<100000:\n",
    "        return \"Medium\"\n",
    "    elif salary>=100000:\n",
    "        return \"High\"\n",
    "    elif salary == None:\n",
    "        return \"Unknown\"\n",
    "\n",
    "salary_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "salary_df = new_df.withColumn(\"Salary_Category\", salary_udf(new_df[\"Salary\"]))\n",
    "salary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4632035-466d-46c9-86d2-d97a1e4cd661",
   "metadata": {},
   "source": [
    "__Convert all columns to string Datatype__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1a48eb2-5365-4899-81d7-f685c57383db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: double (nullable = false)\n",
      " |-- Hire_Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "538d1481-449e-412e-ad19-5710a435d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: string (nullable = false)\n",
      " |-- Hire_Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_change_df = new_df.select([col(column).cast(StringType()).alias(column) for column in df.columns])\n",
    "schema_change_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fa049-7668-42e0-919a-7749dfaa460d",
   "metadata": {},
   "source": [
    "__Convert all column names to lowercase.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16525dbe-6d82-4b55-be39-e0c7ca342c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+----------+\n",
      "| id|   name|department| salary| hire_date|\n",
      "+---+-------+----------+-------+----------+\n",
      "|  1|  Alice|        HR|50000.0|2018-07-10|\n",
      "|  2|    Bob|        IT|40000.0|2017-08-15|\n",
      "|  3|Charlie|   Finance|55000.0|2019-06-25|\n",
      "|  4|  David|        HR|40000.0|2020-09-12|\n",
      "+---+-------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select([col(column).alias(column.lower()) for column in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa83294-555a-4414-9348-ddc8044acbed",
   "metadata": {},
   "source": [
    "__Find duplicate rows and display__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdb4e52b-cfbd-4d17-a6ee-7fe4f2595e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| ID|   Name|Salary|\n",
      "+---+-------+------+\n",
      "|  1|  Alice| 50000|\n",
      "|  2|    Bob| 60000|\n",
      "|  3|Charlie| 55000|\n",
      "|  4|  David| 45000|\n",
      "|  5|    Eve| 70000|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "(1, \"Alice\", 50000),\n",
    "(2, \"Bob\", 60000),\n",
    "(3, \"Charlie\", 55000),\n",
    "(4, \"David\", 45000),\n",
    "(5, \"Eve\", 70000)\n",
    "]\n",
    "columns = [\"ID\", \"Name\", \"Salary\"]\n",
    "df2 = spark.createDataFrame(data, columns)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b161bc35-6a37-4035-8640-09e41b7f7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-----+\n",
      "| ID|Name|Salary|count|\n",
      "+---+----+------+-----+\n",
      "+---+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(df2.columns).count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b807266-5450-49d7-838e-f5c8c6bfb9af",
   "metadata": {},
   "source": [
    "__How to filter valid emails from a list?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84b20660-557d-471f-88be-d614e9579a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| ID|             Email|\n",
      "+---+------------------+\n",
      "|  1| alice@example.com|\n",
      "|  2|       bob@company|\n",
      "|  3|charlie@gmail..com|\n",
      "|  4|  david@domain.org|\n",
      "|  5| invalid-email.com|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"alice@example.com\"),\n",
    "    (2, \"bob@company\"),\n",
    "    (3, \"charlie@gmail..com\"),\n",
    "    (4, \"david@domain.org\"),\n",
    "    (5, \"invalid-email.com\")\n",
    "]\n",
    "\n",
    "df3 = spark.createDataFrame(data, [\"ID\", \"Email\"])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eff8752f-de4e-49c5-95a8-85eac4f0f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| ID|            Email|\n",
      "+---+-----------------+\n",
      "|  1|alice@example.com|\n",
      "|  4| david@domain.org|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_regex = r'^[a-zA-Z0-9_.±]+@[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "valid_emails_df = df3.filter(col(\"Email\").rlike(email_regex))\n",
    "\n",
    "valid_emails_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d121597-7c0b-4ecd-8033-40457cb3c2b9",
   "metadata": {},
   "source": [
    "__How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fad700ee-2b9c-4543-ac96-6c95d7fdd897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|   Name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|  Alice|\n",
      "|  4|Charlie|\n",
      "|  5|    Bob|\n",
      "|  6|  Alice|\n",
      "|  7|  David|\n",
      "|  8|Charlie|\n",
      "|  9|    Eve|\n",
      "| 10|    Bob|\n",
      "| 11|Charlie|\n",
      "| 12|    Eve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\"), (2, \"Bob\"), (3, \"Alice\"), (4, \"Charlie\"), \n",
    "    (5, \"Bob\"), (6, \"Alice\"), (7, \"David\"), (8, \"Charlie\"), \n",
    "    (9, \"Eve\"), (10, \"Bob\"), (11, \"Charlie\"), (12, \"Eve\")\n",
    "]\n",
    "\n",
    "columns = [\"ID\", \"Name\"]\n",
    "df4 = spark.createDataFrame(data, columns)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f9f456f-f7a1-4b3e-9f4e-c53e32372cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e718cf5-8c12-4066-be58-1fcac045bf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', 'Alice']\n"
     ]
    }
   ],
   "source": [
    "name_counts = df4.groupBy(\"Name\").agg(count(\"*\").alias(\"count\"))\n",
    "top_2_names = name_counts.orderBy(col(\"count\").desc()).limit(2)\n",
    "top_2_names_list = [row[\"Name\"] for row in top_2_names.collect()] \n",
    "print(top_2_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c862797-2075-4fba-ae33-70cb429d2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+\n",
      "| ID|   Name|Updated_Name|\n",
      "+---+-------+------------+\n",
      "|  1|  Alice|       Alice|\n",
      "|  2|    Bob|         Bob|\n",
      "|  3|  Alice|       Alice|\n",
      "|  4|Charlie|       Other|\n",
      "|  5|    Bob|         Bob|\n",
      "|  6|  Alice|       Alice|\n",
      "|  7|  David|       Other|\n",
      "|  8|Charlie|       Other|\n",
      "|  9|    Eve|       Other|\n",
      "| 10|    Bob|         Bob|\n",
      "| 11|Charlie|       Other|\n",
      "| 12|    Eve|       Other|\n",
      "+---+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = df4.withColumn(\n",
    "    \"Updated_Name\",\n",
    "    when(col(\"Name\").isin(top_2_names_list), col(\"Name\")).otherwise(\"Other\")\n",
    ")\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76792e-67a7-495b-a4e5-016a45997a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
